{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import sem\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.utils import resample\n","from sklearn.model_selection import StratifiedKFold\n","from numpy import mean, std\n","import numpy as np\n","from sklearn.metrics import roc_auc_score\n","import pandas as pd\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score\n","import seaborn as sns\n","from sklearn.tree import DecisionTreeClassifier\n","from collections import defaultdict\n","from sklearn.metrics import auc\n","from sklearn.neural_network import MLPClassifier\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegression\n","from scipy.stats import sem, t\n","import random"]},{"cell_type":"markdown","id":"b2032bc3-2137-4d22-a62b-006c8fbb1309","metadata":{},"source":["\n","\n","# Coherence-based prediction"]},{"cell_type":"code","execution_count":null,"id":"3057055a-ffc4-4688-b5d3-58e502de85d2","metadata":{},"outputs":[],"source":["seed_value = 777  \n","random.seed(seed_value)\n","np.random.seed(seed_value)"]},{"cell_type":"code","execution_count":null,"id":"72dd69bf-f6e8-4038-af0a-c9a8c756bcdc","metadata":{},"outputs":[],"source":["\n","data = pd.read_csv('D:/coherence_data_150.csv')  # Placeholder path, this won't run here due to no access to the file\n","\n","X = data.drop(columns=['Class', 'ID'])\n","y = data['Class']\n","\n","model_configs = {\n","    \"LogisticRegression\": {\n","        \"model\": LogisticRegression(C=10, max_iter=3000, penalty='l2', solver='lbfgs', random_state=seed_value),\n","        \"n_features\": 115\n","    },\n","    \"GaussianNB\": {\n","        \"model\": GaussianNB(),\n","        \"n_features\": 121\n","    },\n","    \"DecisionTreeClassifier\": {\n","        \"model\": DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=seed_value),\n","        \"n_features\": 32\n","    },\n","    \"RandomForestClassifier\": {\n","        \"model\": RandomForestClassifier(max_depth=10, n_estimators=1000, random_state=seed_value),\n","        \"n_features\": 141\n","    },\n","    \"SVC\": {\n","        \"model\": SVC(C=10, gamma='scale', kernel='rbf', probability=True, random_state=seed_value),\n","        \"n_features\": 143\n","    },\n","    \"MLPClassifier\": {\n","        \"model\": MLPClassifier(hidden_layer_sizes=(50,), max_iter=5000, random_state=seed_value),\n","        \"n_features\": 117\n","    }\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"b926bdc1-2393-434a-b24b-8072ca4fd9ee","metadata":{},"outputs":[],"source":["\n","\n","skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_value)\n","\n","\n","predictions = defaultdict(list)\n","\n","for model_name, config in model_configs.items():\n","    print(f\"Processing {model_name}...\")\n","    \n","    selector = SelectKBest(f_classif, k=config['n_features'])\n","    \n","    pipeline = Pipeline([('selector', selector), ('clf', config['model'])])\n","    \n","    fold_num = 1\n","    for train_index, test_index in skf.split(X, y):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","        \n","        pipeline.fit(X_train, y_train)\n","        \n","        y_pred_proba = pipeline.predict_proba(X_test)\n","        \n","        for idx, actual_id in enumerate(test_index):\n","            predictions[model_name].append({\n","                'ID': data.iloc[actual_id]['ID'],\n","                'Fold': fold_num,\n","                'True_Label': y_test.iloc[idx],\n","                'NCSE_Prob': y_pred_proba[idx][0],\n","                'ME_Prob': y_pred_proba[idx][1],\n","                'BI_Prob': y_pred_proba[idx][2]\n","            })\n","        \n","        fold_num += 1\n","\n","for model_name in model_configs.keys():\n","    predictions[model_name] = pd.DataFrame(predictions[model_name])\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0a0eb436-6238-46cf-9794-3ca157b7c59a","metadata":{},"outputs":[],"source":["for model_name, preds in predictions.items():\n","    file_path = f\"coh_predictions_{model_name}.csv\"\n","    preds.to_csv(file_path, index=False)"]},{"cell_type":"code","execution_count":null,"id":"0aa94dbe-650a-46a4-a4a3-468a8217aa61","metadata":{},"outputs":[],"source":["\n","def plot_roc_curve(y_true, y_pred_proba, model_name):\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(3):\n","        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred_proba[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","    \n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr[0], tpr[0], label=f'NCSE (area = {roc_auc[0]:.2f})')\n","    plt.plot(fpr[1], tpr[1], label=f'ME (area = {roc_auc[1]:.2f})')\n","    plt.plot(fpr[2], tpr[2], label=f'BI (area = {roc_auc[2]:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'ROC Curve - {model_name}')\n","    plt.legend(loc='lower right')\n","    plt.savefig(f'coh_ROC_{model_name}.eps', format='eps')\n","    plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred, model_name):\n","    matrix = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=0, vmax=50)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.savefig(f'coh_CM_{model_name}.eps', format='eps')\n","    plt.show()\n","    \n","\n","def compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba):\n","    cm = confusion_matrix(y_true, y_pred)\n","    \n","    classwise_metrics = []\n","    total_samples = len(y_true)\n","    \n","    for i in range(3):\n","        tp = cm[i, i]\n","        fn = sum(cm[i, :]) - tp\n","        fp = sum(cm[:, i]) - tp\n","        tn = total_samples - (tp + fn + fp)\n","        \n","        accuracy = (tp + tn) / (tp + tn + fp + fn)\n","        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n","        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n","        \n","        classwise_metrics.append({\n","            'accuracy': accuracy,\n","            'f1': f1,\n","            'precision': precision,\n","            'recall': recall,\n","            'tn': tn,\n","            'tp': tp,\n","            'fp': fp,\n","            'fn': fn\n","        })\n","        \n","    overall_accuracy = accuracy_score(y_true, y_pred)\n","    overall_f1 = f1_score(y_true, y_pred, average='macro')\n","    overall_precision = precision_score(y_true, y_pred, average='macro')\n","    overall_recall = recall_score(y_true, y_pred, average='macro')\n","    overall_auc = roc_auc_score(y_true, y_pred_proba, average='macro', multi_class='ovr')\n","    \n","    return {\n","        'overall': {\n","            'accuracy': overall_accuracy,\n","            'f1': overall_f1,\n","            'precision': overall_precision,\n","            'recall': overall_recall,\n","            'auc': overall_auc\n","        },\n","        'classwise': classwise_metrics\n","    }\n","\n","def confidence_interval(data):\n","    n = len(data)\n","    m = mean(data)\n","    std_err = sem(data)\n","    ci = std_err * t.ppf((1 + 0.95) / 2, n - 1)\n","    return (m - ci, m + ci)\n","\n","\n","def bootstrap_ci(y_true, y_pred, y_pred_proba, metric_function, label=None, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the metric using bootstrap.\"\"\"\n","    bootstrap_samples = np.random.choice(len(y_true), size=(n_bootstrap, len(y_true)), replace=True)\n","    y_true_array = np.array(y_true)\n","    \n","    if label is not None:  # For classwise AUC\n","        binary_true = (y_true_array == label).astype(int)\n","        stats = [metric_function(binary_true[indices], y_pred_proba[indices, label]) for indices in bootstrap_samples]\n","    elif y_pred is not None:  # For metrics other than AUC\n","        stats = [metric_function(y_true_array[indices], y_pred[indices]) for indices in bootstrap_samples]\n","    else:  # For overall AUC\n","        stats = [metric_function(y_true_array[indices], y_pred_proba[indices]) for indices in bootstrap_samples]\n","\n","    return (np.percentile(stats, 100 * (alpha / 2.)), np.percentile(stats, 100 * (1 - alpha / 2.)))\n","\n","def bootstrap_ci_for_auc(y_true, y_pred_proba, label, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the AUC using bootstrap for a specific class.\"\"\"\n","    bootstrap_samples = np.random.choice(len(y_true), size=(n_bootstrap, len(y_true)), replace=True)\n","    y_true_array = np.array(y_true)\n","    binary_true = (y_true_array == label).astype(int)\n","    \n","    auc_stats = [roc_auc_score(binary_true[indices], y_pred_proba[indices, label]) for indices in bootstrap_samples]\n","    return (np.percentile(auc_stats, 100 * (alpha / 2.)), np.percentile(auc_stats, 100 * (1 - alpha / 2.)))\n","\n","def bootstrap_ci_classwise_metric(y_true, y_pred, metric_function, label, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the metric using bootstrap for a specific class.\"\"\"\n","    indices = np.arange(len(y_true))  # original indices\n","    \n","    y_true_binary = (np.array(y_true) == label).astype(int)\n","    y_pred_binary = (np.array(y_pred) == label).astype(int)\n","    \n","    stats = []\n","    for _ in range(n_bootstrap):\n","        resampled_indices = resample(indices)\n","        stats.append(metric_function(y_true_binary[resampled_indices], y_pred_binary[resampled_indices]))\n","    \n","    return (np.percentile(stats, 100 * (alpha / 2.)), np.percentile(stats, 100 * (1 - alpha / 2.)))\n","\n","def bootstrap_ci_classwise_auc(y_true, y_pred_proba, label, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the AUC using bootstrap for a specific class.\"\"\"\n","    indices = np.arange(len(y_true))  # original indices\n","    y_true_binary = (np.array(y_true) == label).astype(int)\n","    \n","    auc_stats = []\n","    for _ in range(n_bootstrap):\n","        resampled_indices = resample(indices)\n","        auc_stats.append(roc_auc_score(y_true_binary[resampled_indices], y_pred_proba[resampled_indices, label]))\n","    \n","    return (np.percentile(auc_stats, 100 * (alpha / 2.)), np.percentile(auc_stats, 100 * (1 - alpha / 2.)))\n","\n","def compute_classwise_auc(y_true, y_pred_proba):\n","    class_aucs = []\n","    for i in range(3):\n","        binary_y_true = np.where(y_true == i, 1, 0)\n","        class_aucs.append(roc_auc_score(binary_y_true, y_pred_proba[:, i]))\n","    return class_aucs"]},{"cell_type":"code","execution_count":null,"id":"ff8ce567-1325-4c54-8f18-fdb80169108c","metadata":{},"outputs":[],"source":["performance_data_corrected_v3 = []\n","\n","for model_name, preds in predictions.items():\n","    y_true = preds[\"True_Label\"]\n","    y_pred_proba = preds[[\"NCSE_Prob\", \"ME_Prob\", \"BI_Prob\"]].values\n","    y_pred = np.argmax(y_pred_proba, axis=1)\n","\n","    plot_roc_curve(y_true, y_pred_proba, model_name)\n","    plot_confusion_matrix(y_true, y_pred, model_name)\n","    \n","    metrics_from_cm = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)\n","\n","    for label in range(3):  # For each class label\n","        accuracy_ci = bootstrap_ci_classwise_metric(y_true, y_pred, accuracy_score, label)\n","        f1_ci = bootstrap_ci_classwise_metric(y_true, y_pred, f1_score, label)\n","        precision_ci = bootstrap_ci_classwise_metric(y_true, y_pred, precision_score, label)\n","        recall_ci = bootstrap_ci_classwise_metric(y_true, y_pred, recall_score, label)\n","        auc_ci = bootstrap_ci_classwise_auc(y_true, y_pred_proba, label)\n","     \n","        performance_data_corrected_v3.append({\n","            'Model': model_name,\n","            'Label': label,\n","            'Accuracy': accuracy_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'F1': f1_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Precision': precision_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Recall': recall_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'TP': metrics_from_cm['classwise'][label]['tp'],\n","            'TN': metrics_from_cm['classwise'][label]['tn'],\n","            'FP': metrics_from_cm['classwise'][label]['fp'],\n","            'FN': metrics_from_cm['classwise'][label]['fn'],\n","            'AUC': roc_auc_score((y_true == label).astype(int), y_pred_proba[:, label]),\n","            'Accuracy CI': accuracy_ci,\n","            'F1 CI': f1_ci,\n","            'Precision CI': precision_ci,\n","            'Recall CI': recall_ci,\n","            'AUC CI': auc_ci\n","        })\n","\n","    overall_metrics = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)['overall']\n","    accuracy_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: accuracy_score(yt, yp))\n","    f1_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: f1_score(yt, yp, average='macro'))\n","    precision_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: precision_score(yt, yp, average='macro'))\n","    recall_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: recall_score(yt, yp, average='macro'))\n","    auc_ci = bootstrap_ci(y_true, None, y_pred_proba, lambda yt, yp: roc_auc_score(yt, yp, average='macro', multi_class='ovr'))\n","    \n","    performance_data_corrected_v3.append({\n","        'Model': model_name,\n","        'Label': 'Overall',\n","        'Accuracy': overall_metrics['accuracy'],\n","        'F1': overall_metrics['f1'],\n","        'Precision': overall_metrics['precision'],\n","        'Recall': overall_metrics['recall'],\n","        'AUC': overall_metrics['auc'],\n","        'Accuracy CI': accuracy_ci,\n","        'F1 CI': f1_ci,\n","        'Precision CI': precision_ci,\n","        'Recall CI': recall_ci,\n","        'AUC CI': auc_ci\n","    })\n","    \n","performance_df_corrected_v3 = pd.DataFrame(performance_data_corrected_v3)\n","performance_df_corrected_v3.to_csv('coh_performance_results.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"ed295bb1-76c4-4492-9260-ffb94cbf93fc","metadata":{},"outputs":[],"source":["\n","performance_df = pd.read_csv('coh_performance_results.csv')\n","performance_df"]},{"cell_type":"markdown","id":"20e959ef-37ee-4369-ac93-5a80fc839301","metadata":{},"source":["# Coherence-based prospective validation"]},{"cell_type":"code","execution_count":null,"id":"8fe850dd-81b7-4bce-9bbb-91aba1c7fbb9","metadata":{},"outputs":[],"source":["\n","data = pd.read_csv('D:/coherence_data_150.csv')\n","X = data.drop(columns=['Class', 'ID'])\n","y = data['Class']\n","\n","pro_data = pd.read_csv('D:/coherence_data_pro.csv')\n","X_pro = pro_data.drop(columns=['Class', 'ID'])\n","y_pro = pro_data['Class']\n","\n","predictions_pro = defaultdict(list)\n","\n","for model_name, config in model_configs.items():\n","    print(f\"Processing {model_name}...\")\n","    \n","    selector = SelectKBest(f_classif, k=config['n_features'])\n","    \n","    pipeline = Pipeline([('selector', selector), ('clf', config['model'])])\n","    \n","    pipeline.fit(X, y)\n","    \n","    y_pred_proba_pro = pipeline.predict_proba(X_pro)\n","    \n","    for idx, actual_id in enumerate(pro_data['ID']):\n","        predictions_pro[model_name].append({\n","            'ID': actual_id,\n","            'True_Label': y_pro.iloc[idx],\n","            'NCSE_Prob': y_pred_proba_pro[idx][0],\n","            'ME_Prob': y_pred_proba_pro[idx][1],\n","            'BI_Prob': y_pred_proba_pro[idx][2]\n","        })\n","\n","for model_name in model_configs.keys():\n","    predictions_pro[model_name] = pd.DataFrame(predictions_pro[model_name])\n"]},{"cell_type":"code","execution_count":null,"id":"09e354b2-6ea5-44ef-bbd4-730e65f2f2de","metadata":{},"outputs":[],"source":["for model_name, preds in predictions_pro.items():\n","    file_path = f\"coh_predictions_prospective_{model_name}.csv\"\n","    preds.to_csv(file_path, index=False)"]},{"cell_type":"code","execution_count":null,"id":"61ca5267-24a4-4e93-ba4d-55fe5237fc49","metadata":{},"outputs":[],"source":["\n","def plot_roc_curve(y_true, y_pred_proba, model_name):\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(3):\n","        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred_proba[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","    \n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr[0], tpr[0], label=f'NCSE (area = {roc_auc[0]:.2f})')\n","    plt.plot(fpr[1], tpr[1], label=f'ME (area = {roc_auc[1]:.2f})')\n","    plt.plot(fpr[2], tpr[2], label=f'BI (area = {roc_auc[2]:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'ROC Curve - {model_name}')\n","    plt.legend(loc='lower right')\n","    plt.savefig(f'coh_prospective_ROC_{model_name}.eps', format='eps')\n","    plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred, model_name):\n","    matrix = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=0, vmax=10)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.savefig(f'coh_prospective_CM_{model_name}.eps', format='eps')\n","    plt.show()\n","    \n","performance_data_corrected_pro = []\n","\n","for model_name, preds in predictions_pro.items():\n","    y_true = preds[\"True_Label\"]\n","    y_pred_proba = preds[[\"NCSE_Prob\", \"ME_Prob\", \"BI_Prob\"]].values\n","    y_pred = np.argmax(y_pred_proba, axis=1)\n","\n","    plot_roc_curve(y_true, y_pred_proba, model_name)\n","    plot_confusion_matrix(y_true, y_pred, model_name)\n","    \n","    metrics_from_cm = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)\n","\n","    for label in range(3):  # For each class label\n","        accuracy_ci = bootstrap_ci_classwise_metric(y_true, y_pred, accuracy_score, label)\n","        f1_ci = bootstrap_ci_classwise_metric(y_true, y_pred, f1_score, label)\n","        precision_ci = bootstrap_ci_classwise_metric(y_true, y_pred, precision_score, label)\n","        recall_ci = bootstrap_ci_classwise_metric(y_true, y_pred, recall_score, label)\n","        auc_ci = bootstrap_ci_classwise_auc(y_true, y_pred_proba, label)\n","     \n","        performance_data_corrected_pro.append({\n","            'Model': model_name,\n","            'Label': label,\n","            'Accuracy': accuracy_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'F1': f1_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Precision': precision_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Recall': recall_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'TP': metrics_from_cm['classwise'][label]['tp'],\n","            'TN': metrics_from_cm['classwise'][label]['tn'],\n","            'FP': metrics_from_cm['classwise'][label]['fp'],\n","            'FN': metrics_from_cm['classwise'][label]['fn'],\n","            'AUC': roc_auc_score((y_true == label).astype(int), y_pred_proba[:, label]),\n","            'Accuracy CI': accuracy_ci,\n","            'F1 CI': f1_ci,\n","            'Precision CI': precision_ci,\n","            'Recall CI': recall_ci,\n","            'AUC CI': auc_ci\n","        })\n","\n","    overall_metrics = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)['overall']\n","    accuracy_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: accuracy_score(yt, yp))\n","    f1_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: f1_score(yt, yp, average='macro'))\n","    precision_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: precision_score(yt, yp, average='macro'))\n","    recall_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: recall_score(yt, yp, average='macro'))\n","    auc_ci = bootstrap_ci(y_true, None, y_pred_proba, lambda yt, yp: roc_auc_score(yt, yp, average='macro', multi_class='ovr'))\n","    \n","    performance_data_corrected_pro.append({\n","        'Model': model_name,\n","        'Label': 'Overall',\n","        'Accuracy': overall_metrics['accuracy'],\n","        'F1': overall_metrics['f1'],\n","        'Precision': overall_metrics['precision'],\n","        'Recall': overall_metrics['recall'],\n","        'AUC': overall_metrics['auc'],\n","        'Accuracy CI': accuracy_ci,\n","        'F1 CI': f1_ci,\n","        'Precision CI': precision_ci,\n","        'Recall CI': recall_ci,\n","        'AUC CI': auc_ci\n","    })\n","    \n","performance_data_corrected_pro = pd.DataFrame(performance_data_corrected_pro)\n","performance_data_corrected_pro.to_csv('coh_performance_results_prospective.csv', index=False)\n"]},{"cell_type":"markdown","id":"40c44836-5207-41ab-9e56-187739247aeb","metadata":{},"source":["# Graph measurement-based prediction"]},{"cell_type":"code","execution_count":null,"id":"67dcd534-881b-4fe6-95bb-f6b4a9d4c6ca","metadata":{},"outputs":[],"source":["seed_value = 777  # 원하는 seed 값으로 변경 가능\n","random.seed(seed_value)\n","np.random.seed(seed_value)"]},{"cell_type":"code","execution_count":null,"id":"5151e9a2-2d3b-4b5f-a0d7-91f0d16c65f5","metadata":{},"outputs":[],"source":["\n","data = pd.read_csv('D:/graph_data_150.csv')  # Placeholder path, this won't run here due to no access to the file\n","\n","X = data.drop(columns=['Class', 'ID'])\n","y = data['Class']\n","\n","model_configs = {\n","    \"LogisticRegression\": {\n","        \"model\": LogisticRegression(C=100, max_iter=3000, penalty='l2', solver='lbfgs', random_state=seed_value),\n","        \"n_features\": 12\n","    },\n","    \"GaussianNB\": {\n","        \"model\": GaussianNB(),\n","        \"n_features\": 15\n","    },\n","    \"DecisionTreeClassifier\": {\n","        \"model\": DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=seed_value),\n","        \"n_features\": 60\n","    },\n","    \"RandomForestClassifier\": {\n","        \"model\": RandomForestClassifier(max_depth=10, n_estimators=100, random_state=seed_value),\n","        \"n_features\": 39\n","    },\n","    \"SVC\": {\n","        \"model\": SVC(C=10, gamma='scale', kernel='linear', probability=True, random_state=seed_value),\n","        \"n_features\": 12\n","    },\n","    \"MLPClassifier\": {\n","        \"model\": MLPClassifier(hidden_layer_sizes=(10,), max_iter=5000, random_state=seed_value),\n","        \"n_features\": 12\n","    }\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"2bcb68d4-7c97-47ca-8bd0-2512eb91c904","metadata":{},"outputs":[],"source":["\n","\n","skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_value)\n","\n","\n","predictions = defaultdict(list)\n","\n","for model_name, config in model_configs.items():\n","    print(f\"Processing {model_name}...\")\n","    \n","    selector = SelectKBest(f_classif, k=config['n_features'])\n","    \n","    pipeline = Pipeline([('selector', selector), ('clf', config['model'])])\n","    \n","    fold_num = 1\n","    for train_index, test_index in skf.split(X, y):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","        \n","        pipeline.fit(X_train, y_train)\n","        \n","        y_pred_proba = pipeline.predict_proba(X_test)\n","        \n","        for idx, actual_id in enumerate(test_index):\n","            predictions[model_name].append({\n","                'ID': data.iloc[actual_id]['ID'],\n","                'Fold': fold_num,\n","                'True_Label': y_test.iloc[idx],\n","                'NCSE_Prob': y_pred_proba[idx][0],\n","                'ME_Prob': y_pred_proba[idx][1],\n","                'BI_Prob': y_pred_proba[idx][2]\n","            })\n","        \n","        fold_num += 1\n","\n","for model_name in model_configs.keys():\n","    predictions[model_name] = pd.DataFrame(predictions[model_name])\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6ac2de16-df07-40ee-a2db-f53d524cd4f7","metadata":{},"outputs":[],"source":["for model_name, preds in predictions.items():\n","    file_path = f\"graph_predictions_{model_name}.csv\"\n","    preds.to_csv(file_path, index=False)"]},{"cell_type":"code","execution_count":null,"id":"ca790484-ecf8-4235-bfc3-2b642642af9e","metadata":{},"outputs":[],"source":["\n","def plot_roc_curve(y_true, y_pred_proba, model_name):\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(3):\n","        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred_proba[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","    \n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr[0], tpr[0], label=f'NCSE (area = {roc_auc[0]:.2f})')\n","    plt.plot(fpr[1], tpr[1], label=f'ME (area = {roc_auc[1]:.2f})')\n","    plt.plot(fpr[2], tpr[2], label=f'BI (area = {roc_auc[2]:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'ROC Curve - {model_name}')\n","    plt.legend(loc='lower right')\n","    plt.savefig(f'graph_ROC_{model_name}.eps', format='eps')\n","    plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred, model_name):\n","    matrix = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=0, vmax=50)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.savefig(f'graph_CM_{model_name}.eps', format='eps')\n","    plt.show()\n","    \n","\n","def compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba):\n","    cm = confusion_matrix(y_true, y_pred)\n","    \n","    classwise_metrics = []\n","    total_samples = len(y_true)\n","    \n","    for i in range(3):\n","        tp = cm[i, i]\n","        fn = sum(cm[i, :]) - tp\n","        fp = sum(cm[:, i]) - tp\n","        tn = total_samples - (tp + fn + fp)\n","        \n","        accuracy = (tp + tn) / (tp + tn + fp + fn)\n","        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n","        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n","        \n","        classwise_metrics.append({\n","            'accuracy': accuracy,\n","            'f1': f1,\n","            'precision': precision,\n","            'recall': recall,\n","            'tn': tn,\n","            'tp': tp,\n","            'fp': fp,\n","            'fn': fn\n","        })\n","        \n","    overall_accuracy = accuracy_score(y_true, y_pred)\n","    overall_f1 = f1_score(y_true, y_pred, average='macro')\n","    overall_precision = precision_score(y_true, y_pred, average='macro')\n","    overall_recall = recall_score(y_true, y_pred, average='macro')\n","    overall_auc = roc_auc_score(y_true, y_pred_proba, average='macro', multi_class='ovr')\n","    \n","    return {\n","        'overall': {\n","            'accuracy': overall_accuracy,\n","            'f1': overall_f1,\n","            'precision': overall_precision,\n","            'recall': overall_recall,\n","            'auc': overall_auc\n","        },\n","        'classwise': classwise_metrics\n","    }\n","\n","def confidence_interval(data):\n","    n = len(data)\n","    m = mean(data)\n","    std_err = sem(data)\n","    ci = std_err * t.ppf((1 + 0.95) / 2, n - 1)\n","    return (m - ci, m + ci)\n","\n","\n","def bootstrap_ci(y_true, y_pred, y_pred_proba, metric_function, label=None, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the metric using bootstrap.\"\"\"\n","    bootstrap_samples = np.random.choice(len(y_true), size=(n_bootstrap, len(y_true)), replace=True)\n","    y_true_array = np.array(y_true)\n","    \n","    if label is not None:  # For classwise AUC\n","        binary_true = (y_true_array == label).astype(int)\n","        stats = [metric_function(binary_true[indices], y_pred_proba[indices, label]) for indices in bootstrap_samples]\n","    elif y_pred is not None:  # For metrics other than AUC\n","        stats = [metric_function(y_true_array[indices], y_pred[indices]) for indices in bootstrap_samples]\n","    else:  # For overall AUC\n","        stats = [metric_function(y_true_array[indices], y_pred_proba[indices]) for indices in bootstrap_samples]\n","\n","    return (np.percentile(stats, 100 * (alpha / 2.)), np.percentile(stats, 100 * (1 - alpha / 2.)))\n","\n","def bootstrap_ci_for_auc(y_true, y_pred_proba, label, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the AUC using bootstrap for a specific class.\"\"\"\n","    bootstrap_samples = np.random.choice(len(y_true), size=(n_bootstrap, len(y_true)), replace=True)\n","    y_true_array = np.array(y_true)\n","    binary_true = (y_true_array == label).astype(int)\n","    \n","    auc_stats = [roc_auc_score(binary_true[indices], y_pred_proba[indices, label]) for indices in bootstrap_samples]\n","    return (np.percentile(auc_stats, 100 * (alpha / 2.)), np.percentile(auc_stats, 100 * (1 - alpha / 2.)))\n","\n","def bootstrap_ci_classwise_metric(y_true, y_pred, metric_function, label, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the metric using bootstrap for a specific class.\"\"\"\n","    indices = np.arange(len(y_true))  # original indices\n","    \n","    y_true_binary = (np.array(y_true) == label).astype(int)\n","    y_pred_binary = (np.array(y_pred) == label).astype(int)\n","    \n","    stats = []\n","    for _ in range(n_bootstrap):\n","        resampled_indices = resample(indices)\n","        stats.append(metric_function(y_true_binary[resampled_indices], y_pred_binary[resampled_indices]))\n","    \n","    return (np.percentile(stats, 100 * (alpha / 2.)), np.percentile(stats, 100 * (1 - alpha / 2.)))\n","\n","def bootstrap_ci_classwise_auc(y_true, y_pred_proba, label, n_bootstrap=1000, alpha=0.05):\n","    \"\"\"Compute the (1-alpha) confidence interval of the AUC using bootstrap for a specific class.\"\"\"\n","    indices = np.arange(len(y_true))  # original indices\n","    y_true_binary = (np.array(y_true) == label).astype(int)\n","    \n","    auc_stats = []\n","    for _ in range(n_bootstrap):\n","        resampled_indices = resample(indices)\n","        auc_stats.append(roc_auc_score(y_true_binary[resampled_indices], y_pred_proba[resampled_indices, label]))\n","    \n","    return (np.percentile(auc_stats, 100 * (alpha / 2.)), np.percentile(auc_stats, 100 * (1 - alpha / 2.)))\n","\n","def compute_classwise_auc(y_true, y_pred_proba):\n","    class_aucs = []\n","    for i in range(3):\n","        binary_y_true = np.where(y_true == i, 1, 0)\n","        class_aucs.append(roc_auc_score(binary_y_true, y_pred_proba[:, i]))\n","    return class_aucs"]},{"cell_type":"code","execution_count":null,"id":"b3d7b93d-a23b-4507-99c2-d0674f00a10b","metadata":{},"outputs":[],"source":["performance_data_corrected_v3 = []\n","\n","for model_name, preds in predictions.items():\n","    y_true = preds[\"True_Label\"]\n","    y_pred_proba = preds[[\"NCSE_Prob\", \"ME_Prob\", \"BI_Prob\"]].values\n","    y_pred = np.argmax(y_pred_proba, axis=1)\n","\n","    plot_roc_curve(y_true, y_pred_proba, model_name)\n","    plot_confusion_matrix(y_true, y_pred, model_name)\n","    \n","    metrics_from_cm = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)\n","\n","    for label in range(3):  # For each class label\n","        accuracy_ci = bootstrap_ci_classwise_metric(y_true, y_pred, accuracy_score, label)\n","        f1_ci = bootstrap_ci_classwise_metric(y_true, y_pred, f1_score, label)\n","        precision_ci = bootstrap_ci_classwise_metric(y_true, y_pred, precision_score, label)\n","        recall_ci = bootstrap_ci_classwise_metric(y_true, y_pred, recall_score, label)\n","        auc_ci = bootstrap_ci_classwise_auc(y_true, y_pred_proba, label)\n","     \n","        performance_data_corrected_v3.append({\n","            'Model': model_name,\n","            'Label': label,\n","            'Accuracy': accuracy_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'F1': f1_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Precision': precision_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Recall': recall_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'TP': metrics_from_cm['classwise'][label]['tp'],\n","            'TN': metrics_from_cm['classwise'][label]['tn'],\n","            'FP': metrics_from_cm['classwise'][label]['fp'],\n","            'FN': metrics_from_cm['classwise'][label]['fn'],\n","            'AUC': roc_auc_score((y_true == label).astype(int), y_pred_proba[:, label]),\n","            'Accuracy CI': accuracy_ci,\n","            'F1 CI': f1_ci,\n","            'Precision CI': precision_ci,\n","            'Recall CI': recall_ci,\n","            'AUC CI': auc_ci\n","        })\n","\n","    overall_metrics = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)['overall']\n","    accuracy_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: accuracy_score(yt, yp))\n","    f1_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: f1_score(yt, yp, average='macro'))\n","    precision_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: precision_score(yt, yp, average='macro'))\n","    recall_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: recall_score(yt, yp, average='macro'))\n","    auc_ci = bootstrap_ci(y_true, None, y_pred_proba, lambda yt, yp: roc_auc_score(yt, yp, average='macro', multi_class='ovr'))\n","    \n","    performance_data_corrected_v3.append({\n","        'Model': model_name,\n","        'Label': 'Overall',\n","        'Accuracy': overall_metrics['accuracy'],\n","        'F1': overall_metrics['f1'],\n","        'Precision': overall_metrics['precision'],\n","        'Recall': overall_metrics['recall'],\n","        'AUC': overall_metrics['auc'],\n","        'Accuracy CI': accuracy_ci,\n","        'F1 CI': f1_ci,\n","        'Precision CI': precision_ci,\n","        'Recall CI': recall_ci,\n","        'AUC CI': auc_ci\n","    })\n","    \n","performance_df_corrected_v3 = pd.DataFrame(performance_data_corrected_v3)\n","performance_df_corrected_v3.to_csv('graph_performance_results.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"0aad9f2a-587f-491e-9c23-89eb8ccdba05","metadata":{},"outputs":[],"source":["performance_df = pd.read_csv('formatted_performance_results_prospective_.csv')\n","performance_df"]},{"cell_type":"markdown","id":"f43817f3-6d53-4f25-866c-a8fbc7b760fd","metadata":{},"source":["# # Graph measurement-based prospective validation"]},{"cell_type":"code","execution_count":null,"id":"257e88ca-2652-46fc-a8a7-97d01b99ad5a","metadata":{},"outputs":[],"source":["\n","data = pd.read_csv('D:/graph_data_150.csv')\n","X = data.drop(columns=['Class', 'ID'])\n","y = data['Class']\n","\n","pro_data = pd.read_csv('D:/graph_data_pro.csv')\n","X_pro = pro_data.drop(columns=['Class', 'ID'])\n","y_pro = pro_data['Class']\n","\n","predictions_pro = defaultdict(list)\n","\n","for model_name, config in model_configs.items():\n","    print(f\"Processing {model_name}...\")\n","    \n","    selector = SelectKBest(f_classif, k=config['n_features'])\n","    \n","    pipeline = Pipeline([('selector', selector), ('clf', config['model'])])\n","    \n","    pipeline.fit(X, y)\n","    \n","    y_pred_proba_pro = pipeline.predict_proba(X_pro)\n","    \n","    for idx, actual_id in enumerate(pro_data['ID']):\n","        predictions_pro[model_name].append({\n","            'ID': actual_id,\n","            'True_Label': y_pro.iloc[idx],\n","            'NCSE_Prob': y_pred_proba_pro[idx][0],\n","            'ME_Prob': y_pred_proba_pro[idx][1],\n","            'BI_Prob': y_pred_proba_pro[idx][2]\n","        })\n","\n","for model_name in model_configs.keys():\n","    predictions_pro[model_name] = pd.DataFrame(predictions_pro[model_name])\n"]},{"cell_type":"code","execution_count":null,"id":"d099408a-66b8-4486-b85a-0ce2f324bbff","metadata":{},"outputs":[],"source":["for model_name, preds in predictions_pro.items():\n","    file_path = f\"graph_predictions_prospective_{model_name}.csv\"\n","    preds.to_csv(file_path, index=False)"]},{"cell_type":"code","execution_count":null,"id":"b0ded23d-9554-45c6-aec5-e22746713186","metadata":{},"outputs":[],"source":["\n","def plot_roc_curve(y_true, y_pred_proba, model_name):\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(3):\n","        fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred_proba[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","    \n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr[0], tpr[0], label=f'NCSE (area = {roc_auc[0]:.2f})')\n","    plt.plot(fpr[1], tpr[1], label=f'ME (area = {roc_auc[1]:.2f})')\n","    plt.plot(fpr[2], tpr[2], label=f'BI (area = {roc_auc[2]:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'ROC Curve - {model_name}')\n","    plt.legend(loc='lower right')\n","    plt.savefig(f'graph_prospective_ROC_{model_name}.eps', format='eps')\n","    plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred, model_name):\n","    matrix = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=0, vmax=10)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.savefig(f'graph_prospective_CM_{model_name}.eps', format='eps')\n","    plt.show()\n","    \n","performance_data_corrected_pro = []\n","\n","for model_name, preds in predictions_pro.items():\n","    y_true = preds[\"True_Label\"]\n","    y_pred_proba = preds[[\"NCSE_Prob\", \"ME_Prob\", \"BI_Prob\"]].values\n","    y_pred = np.argmax(y_pred_proba, axis=1)\n","\n","    plot_roc_curve(y_true, y_pred_proba, model_name)\n","    plot_confusion_matrix(y_true, y_pred, model_name)\n","    \n","    metrics_from_cm = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)\n","\n","    for label in range(3):  # For each class label\n","        accuracy_ci = bootstrap_ci_classwise_metric(y_true, y_pred, accuracy_score, label)\n","        f1_ci = bootstrap_ci_classwise_metric(y_true, y_pred, f1_score, label)\n","        precision_ci = bootstrap_ci_classwise_metric(y_true, y_pred, precision_score, label)\n","        recall_ci = bootstrap_ci_classwise_metric(y_true, y_pred, recall_score, label)\n","        auc_ci = bootstrap_ci_classwise_auc(y_true, y_pred_proba, label)\n","     \n","        performance_data_corrected_pro.append({\n","            'Model': model_name,\n","            'Label': label,\n","            'Accuracy': accuracy_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'F1': f1_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Precision': precision_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'Recall': recall_score((y_true == label).astype(int), (y_pred == label).astype(int)),\n","            'TP': metrics_from_cm['classwise'][label]['tp'],\n","            'TN': metrics_from_cm['classwise'][label]['tn'],\n","            'FP': metrics_from_cm['classwise'][label]['fp'],\n","            'FN': metrics_from_cm['classwise'][label]['fn'],\n","            'AUC': roc_auc_score((y_true == label).astype(int), y_pred_proba[:, label]),\n","            'Accuracy CI': accuracy_ci,\n","            'F1 CI': f1_ci,\n","            'Precision CI': precision_ci,\n","            'Recall CI': recall_ci,\n","            'AUC CI': auc_ci\n","        })\n","\n","    overall_metrics = compute_corrected_metrics_from_cm(y_true, y_pred, y_pred_proba)['overall']\n","    accuracy_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: accuracy_score(yt, yp))\n","    f1_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: f1_score(yt, yp, average='macro'))\n","    precision_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: precision_score(yt, yp, average='macro'))\n","    recall_ci = bootstrap_ci(y_true, y_pred, None, lambda yt, yp: recall_score(yt, yp, average='macro'))\n","    auc_ci = bootstrap_ci(y_true, None, y_pred_proba, lambda yt, yp: roc_auc_score(yt, yp, average='macro', multi_class='ovr'))\n","    \n","    performance_data_corrected_pro.append({\n","        'Model': model_name,\n","        'Label': 'Overall',\n","        'Accuracy': overall_metrics['accuracy'],\n","        'F1': overall_metrics['f1'],\n","        'Precision': overall_metrics['precision'],\n","        'Recall': overall_metrics['recall'],\n","        'AUC': overall_metrics['auc'],\n","        'Accuracy CI': accuracy_ci,\n","        'F1 CI': f1_ci,\n","        'Precision CI': precision_ci,\n","        'Recall CI': recall_ci,\n","        'AUC CI': auc_ci\n","    })\n","    \n","performance_data_corrected_pro = pd.DataFrame(performance_data_corrected_pro)\n","performance_data_corrected_pro.to_csv('graph_performance_results_prospective.csv', index=False)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
